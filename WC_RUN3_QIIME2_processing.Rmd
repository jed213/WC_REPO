---
title: "WC RUN3 QIIME2 processing"
author: "Jess Diaz"
date: "2023-10-02"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Step 1: Import sequencing files

In order to properly use the sequencing files in QIIME2, I first need to download all the sequencing files from the Kohl Lab BaseSpace account and upload them into the cluster into one folder.

Downloading the run3 (ITS) files was done by Jose and the files were deposited into the shared OneDrive - both the initial run and samples they resequenced. I then uploaded these files into the cluster using the ondemand user interface into mainITS. Jose had already defoldered these.  

In the end, there were 124 files from 62 samples (57 samples + 3 extraction controls + 2 PCR controls, excluding the redos) for Run 3.

## Step 2: Generate manifest file

To import the files I needed to have a manifest file which associates each fastq.gz file with a sample name and whether it is a forward or reverse file. Instructions for this were in Elizabeth's *BaseSpace Protocol* Word Doc.  
The final file name is called *WCRUNITS-pe-33-manifest* and saved as a .csv. I then uploaded it to the cluster directory where the sequence files are.

## Using QIIME2 in the cluster

The following code is reference for loading qiime2 in the cluster and running an interactive job. I used Qiime2 version 2023.5.

```{bash cluster setup, eval = FALSE}
# load qiime2
module load qiime2/2023.5

# check it's working
qiime --help

# run interactive job
srun --pty bash

# submit normal job
sbatch [job script]
```

## Step 3: Import samples to QIIME2

Instructions for this were in Elizabeth's *BaseSpace Protocol* Word Doc.

```{bash import to qiime2, eval = FALSE}
# first, cd to file directory
# import samples using manifest file
qiime tools import \
  --type 'SampleData[PairedEndSequencesWithQuality]' \
  --input-path WCRUNITS-pe-33-manifest.csv \
  --output-path paired-end-demux.qza \
  --input-format PairedEndFastqManifestPhred33
  
# make sure it ran correctly
qiime tools peek paired-end-demux.qza
```

These files have been imported into QIIME2 as Sample Data (paired end with quality info), of the data format SingleLanePerSamplePairedEndFastqDirFmt.

## Step 4: Sequence trimming

Instructions for this were in Elizabeth's *BaseSpace Protocol* Word Doc.  

Now that the sequences have been uploaded into QIIME2 in a useable format, we can look at the sequence length and quality in order to know how to trim the data.  

```{bash visualize quality, eval = FALSE}
# visualize reads
qiime demux summarize \
  --i-data paired-end-demux.qza \
  --o-visualization paired-end-demux.qzv
  
# download .qzv file and view through view.qiime2.org
```

The sequencing facility did not remove the primers (ITS1F (CTTGGTCATTTAGAGGAAGTAA) ITS2R (GCTGCGTTCTTCATCGATGC)). Based on that and the sequence quality, I trimmed using the following parameters. 220 may be a bit lenient, but I can always trim more if something seems off. That is about when quality scores drop below 20. I did this through a bash script instead of an interactive job. 

```{bash dada2, eval = FALSE}
#!/bin/bash
#SBATCH --partition=htc
#SBATCH --job-name=dada2.X
#SBATCH --output=outs/dada2.X.out
#SBATCH --error=errs/dada2.X.err
#SBATCH --time=0-10:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=jed213@pitt.edu

module load qiime2/2023.5

# trim reads based on sequence length and quality
qiime dada2 denoise-paired \
  --i-demultiplexed-seqs paired-end-demux.qza \
  --o-table table.qza \
  --o-representative-sequences rep-seqs.qza \
  --p-n-threads 0 \
  --verbose \
  --p-trunc-len-f 220 \
  --p-trunc-len-r 220 \
  --p-trim-left-f 22 \
  --p-trim-left-r 20 \
  --o-denoising-stats denoising-stats.qza

# visualize number of reads that passed each filter at each step
qiime metadata tabulate \
  --m-input-file denoising-stats.qza \
  --o-visualization denoising-stats.qzv

# download .qzv file and view through view.qiime2.org
```

Running this gives two important file outputs: table.qza and rep-seqs.qza. It also gives a stats output. At this point we can look at stats from this project, and visualize the number of reads that passed each filter at each step. 

I am pretty happy with how the blanks and samples look here. Blanks (extraction and PCR) had <20% reads pass (many had 0), and the samples tend to have around 70-80%.

## Step 5: Set up metadata file

The next steps will require a QIIME2-compatible metadata file. Instructions on proper format are here https://gregcaporaso.github.io/q2book/using/metadata.html. Note that sample names should be in the exact format as they were in the manifest file. Also, any samples not included here will be discarded from the rest of the analysis (so include only the desired copy of any samples that were sequenced twice, and make sure to include blanks). My file is *WC_qiime_ITS_metadata.txt*. There are 62 samples total including blanks.

## More visualization

We can now visualize number of features (reads) per sample. This should match what was in denoising-stats.qzv.

```{bash visualize reads, eval = FALSE}
# visualize number of features per sample
qiime feature-table summarize \
  --i-table table.qza \
  --o-visualization table.qzv \
  --m-sample-metadata-file WC_qiime_ITS_metadata.txt
  
# download .qzv file and view through view.qiime2.org
```

We can also look at what the specific sequences were for each feature. Typically not needed.

```{bash visualize sequences, eval = FALSE}
# visualize sequences for each feature
qiime feature-table tabulate-seqs \
  --i-data rep-seqs.qza \
  --o-visualization rep-seqs.qzv

# download .qzv file and view through view.qiime2.org
```

## Step 7: Create phylogenetic trees and classify taxonomy

In order to do other steps, we need a phylogeny for these samples so that we can then assign taxa to the specific features in this dataset. Instead of training it myself (I may do this in future), I will be using a pre-trained classifier found here https://github.com/colinbrislawn/unite-train/releases.

This should also be run through a bash script. Note, this script should be run from outside the main folder, not within it.

